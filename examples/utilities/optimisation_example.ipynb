{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Optimisation example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def f_rosenbrock(x, grad):\n",
        "    \"\"\"\n",
        "    The rosenbrock function has a single optimum at:\n",
        "        f(a, a**2) = 0\n",
        "    \"\"\"\n",
        "    a = 1\n",
        "    b = 100\n",
        "    value = (a - x[0]) ** 2 + b * (x[1] - x[0] ** 2) ** 2\n",
        "\n",
        "    if grad.size > 0:\n",
        "        # Here we can calculate the gradient of the objective function\n",
        "        # NOTE: Gradients and constraints must be assigned in-place\n",
        "        grad[0] = -2 * a + 4 * b * x[0] ** 3 - 4 * b * x[0] * x[1] + 2 * x[0]\n",
        "        grad[1] = 2 * b * (x[1] - x[0] ** 2)\n",
        "\n",
        "    return value\n",
        "\n",
        "\n",
        "results = {}\n",
        "for algorithm in [\"SLSQP\", \"COBYLA\", \"ISRES\"]:\n",
        "    optimiser = Optimiser(\n",
        "        algorithm, 2, opt_conditions={\"ftol_rel\": 1e-22, \"ftol_abs\": 1e-12}\n",
        "    )\n",
        "    optimiser.set_objective_function(f_rosenbrock)\n",
        "    optimiser.set_lower_bounds([-2, -2])\n",
        "    optimiser.set_upper_bounds([3, 3])\n",
        "    result = optimiser.optimise([0.5, -0.5])\n",
        "    results[algorithm] = {\n",
        "        \"x\": result,\n",
        "        \"f(x)\": optimiser.optimum_value,\n",
        "        \"n_evals\": optimiser.n_evals,\n",
        "    }\n",
        "\n",
        "print(\"Rosenbrock results:\")\n",
        "pprint(results)\n",
        "\n",
        "# They all get pretty close to the optimum here.\n",
        "\n",
        "# The SLSQP algorithm which leverages the gradient we (in this case not so painfully)\n",
        "# derived, does better here. It finds the optimum exactly, and in very few iterations.\n",
        "\n",
        "\n",
        "# Now let's add in a constraint\n",
        "\n",
        "\n",
        "def f_constraint(constraint, x, grad):\n",
        "    \"\"\"\n",
        "    Let's say that we only want to search the space in which some combinations of\n",
        "    variables are not allowed. We can't implement this using just bounds, hence we need\n",
        "    to add some constraints in.\n",
        "\n",
        "    All we're effectively doing here is chopping the search space rectangle, and saying\n",
        "    that:\n",
        "        x1 + x2 < 3\n",
        "        x2 - 2x1 > 1\n",
        "    \"\"\"\n",
        "    constraint[0] = (x[0] + x[1]) - 3\n",
        "    constraint[1] = (-2 * x[0] + x[1]) + 1\n",
        "\n",
        "    if grad.size > 0:\n",
        "        # Again, if we can easily calculate the gradients.. we should!\n",
        "        grad[0, :] = np.array([1, 1])\n",
        "        grad[1, :] = np.array([-2, 1])\n",
        "\n",
        "    return constraint\n",
        "\n",
        "\n",
        "results = {}\n",
        "for algorithm in [\"SLSQP\", \"COBYLA\", \"ISRES\"]:\n",
        "    optimiser = Optimiser(\n",
        "        algorithm,\n",
        "        2,\n",
        "        opt_conditions={\"ftol_rel\": 1e-22, \"ftol_abs\": 1e-12, \"max_eval\": 1000},\n",
        "    )\n",
        "    optimiser.set_objective_function(f_rosenbrock)\n",
        "    optimiser.set_lower_bounds([-2, -2])\n",
        "    optimiser.set_upper_bounds([3, 3])\n",
        "    optimiser.add_ineq_constraints(f_constraint, tolerance=1e-6 * np.ones(2))\n",
        "    result = optimiser.optimise([0.5, -0.5])\n",
        "    results[algorithm] = {\n",
        "        \"x\": result,\n",
        "        \"f(x)\": optimiser.optimum_value,\n",
        "        \"n_evals\": optimiser.n_evals,\n",
        "    }\n",
        "\n",
        "print(\"Constrained Rosenbrock results\")\n",
        "pprint(results)\n",
        "\n",
        "# So SLSQP and COBYLA do fine here, because there is only one minimum and it is a problem\n",
        "# well suited to these algorithms. Note that the optimum complies with the constraints,\n",
        "# so these algorithms actually perform better with the constraints (there is less space\n",
        "# to search, and more rules and gradients to leverage).\n",
        "\n",
        "# ISRES probably won't do so well here on average. Note that every time you run ISRES,\n",
        "# you will get different results unless you set the same random seed.\n",
        "\n",
        "# It's usually wise to set a max_eval termination condition when doing optimisations,\n",
        "# otherwise they can take a very long time... and may never converge.\n",
        "\n",
        "# What about a strongly multi-modal function with no easy analytical gradient?\n",
        "\n",
        "\n",
        "def f_eggholder(x):\n",
        "    \"\"\"\n",
        "    The multi-dimensional Eggholder function. It is strongly multi-modal.\n",
        "\n",
        "    For the 2-D case bounded at +/- 512, the optimum is at:\n",
        "        f(512, 404.2319..) = -959.6407..\n",
        "    \"\"\"\n",
        "    f_x = 0\n",
        "    for i in range(len(x) - 1):\n",
        "        f_x += -(x[i + 1] + 47) * np.sin(np.sqrt(abs(x[i + 1] + 0.5 * x[i] + 47))) - x[\n",
        "            i\n",
        "        ] * np.sin(np.sqrt(abs(x[0] - x[i + 1] - 47)))\n",
        "    return f_x\n",
        "\n",
        "\n",
        "def f_eggholder_objective(x, grad):\n",
        "    \"\"\"\n",
        "    Our little wrapper to interface with the optimiser (which needs a grad\n",
        "    argument).\n",
        "    \"\"\"\n",
        "    value = f_eggholder(x)\n",
        "\n",
        "    if grad.size > 0:\n",
        "        # Here, SLSQP needs to know what the gradient of the objective function is..\n",
        "        # Seeing as we are lazy, we're going to approximate it.\n",
        "        # This is not particularly robust, and can cause headaches.\n",
        "        grad[:] = approx_derivative(f_eggholder, x, f0=value)\n",
        "\n",
        "    return value\n",
        "\n",
        "\n",
        "results = {}\n",
        "for algorithm in [\"SLSQP\", \"COBYLA\", \"ISRES\"]:\n",
        "    optimiser = Optimiser(\n",
        "        algorithm,\n",
        "        2,\n",
        "        opt_conditions={\"ftol_rel\": 1e-22, \"ftol_abs\": 1e-12, \"max_eval\": 10000},\n",
        "    )\n",
        "    optimiser.set_objective_function(f_eggholder_objective)\n",
        "    optimiser.set_lower_bounds([-512, -512])\n",
        "    optimiser.set_upper_bounds([512, 512])\n",
        "    result = optimiser.optimise([0, 0])\n",
        "    results[algorithm] = {\n",
        "        \"x\": result,\n",
        "        \"f(x)\": optimiser.optimum_value,\n",
        "        \"n_evals\": optimiser.n_evals,\n",
        "    }\n",
        "\n",
        "print(\"Eggholder results:\")\n",
        "pprint(results)\n",
        "\n",
        "# SLSQP and COBYLA are local optimisation algorithms, and converge rapidly on a local\n",
        "# minimum. ISRES is a stochastic global optimisation algorithm, and keeps looking for\n",
        "# longer, finding a much better minimum, but caps out at the maximum number of\n",
        "# evaluations (usually)."
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
