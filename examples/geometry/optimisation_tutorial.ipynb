{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "We're going to set up some geometry optimisation problems and solve them with different\n",
        "optimisastion algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "from bluemira.geometry.optimisation import GeometryOptimisationProblem, minimise_length\n",
        "from bluemira.geometry.parameterisations import PrincetonD\n",
        "from bluemira.utilities.opt_problems import OptimisationConstraint, OptimisationObjective\n",
        "from bluemira.utilities.optimiser import Optimiser, approx_derivative\n",
        "from bluemira.utilities.tools import set_random_seed"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's set up a simple GeometryOptimisationProblem, where we minimise the length of\n",
        "parameterised geometry.\n",
        "\n",
        "First, we set up the GeometryParameterisation, with some bounds on the variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parameterisation_1 = PrincetonD(\n",
        "    {\n",
        "        \"x1\": {\"lower_bound\": 2, \"value\": 4, \"upper_bound\": 6},\n",
        "        \"x2\": {\"lower_bound\": 10, \"value\": 14, \"upper_bound\": 16},\n",
        "        \"dz\": {\"lower_bound\": -0.5, \"value\": 0, \"upper_bound\": 0.5},\n",
        "    }\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we're minimising the length, and we can work out that the dz variable will not\n",
        "affect the optimisation, so let's just fix at some value and remove it from the problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parameterisation_1.fix_variable(\"dz\", value=0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Now, we set up our optimiser. We'll start with a gradient-based optimisation algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "slsqp_optimiser = Optimiser(\n",
        "    \"SLSQP\", opt_conditions={\"max_eval\": 100, \"ftol_abs\": 1e-12, \"ftol_rel\": 1e-12}\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Next, we make our objective function, using in this case one of the ready-made ones.\n",
        "\n",
        "NOTE: This `minimise_length` function includes automatic numerical calculation of the\n",
        "objective function gradient, and expects a certain signature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "objective = OptimisationObjective(\n",
        "    minimise_length,\n",
        "    f_objective_args={\"parameterisation\": parameterisation_1},\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Finally, we initialise our `GeometryOptimisationProblem` and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "my_problem = GeometryOptimisationProblem(parameterisation_1, slsqp_optimiser, objective)\n",
        "my_problem.optimise()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Here we're minimising the length, within the bounds of our PrincetonD parameterisation,\n",
        "so we'd expect that x1 goes to its upper bound, and x2 goes to its lower bound."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    f\"x1: value: {parameterisation_1.variables['x1'].value}, upper_bound: {parameterisation_1.variables['x1'].upper_bound}\"\n",
        ")\n",
        "print(\n",
        "    f\"x2: value: {parameterisation_1.variables['x2'].value}, lower_bound: {parameterisation_1.variables['x2'].lower_bound}\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's do the same with an optimisation algorithm that doesn't require gradients.\n",
        "The `minimise_length` function will not calculate the gradients numerically if the\n",
        "optimisation algorithm does not require them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parameterisation_2 = PrincetonD(\n",
        "    {\n",
        "        \"x1\": {\"lower_bound\": 2, \"value\": 4, \"upper_bound\": 6},\n",
        "        \"x2\": {\"lower_bound\": 10, \"value\": 14, \"upper_bound\": 16},\n",
        "        \"dz\": {\"lower_bound\": -0.5, \"value\": 0, \"upper_bound\": 0.5},\n",
        "    }\n",
        ")\n",
        "\n",
        "cobyla_optimiser = Optimiser(\n",
        "    \"COBYLA\",\n",
        "    opt_conditions={\n",
        "        \"ftol_rel\": 1e-3,\n",
        "        \"xtol_rel\": 1e-12,\n",
        "        \"xtol_abs\": 1e-12,\n",
        "        \"max_eval\": 1000,\n",
        "    },\n",
        ")\n",
        "objective = OptimisationObjective(\n",
        "    minimise_length,\n",
        "    f_objective_args={\"parameterisation\": parameterisation_2},\n",
        ")\n",
        "problem = GeometryOptimisationProblem(parameterisation_2, cobyla_optimiser, objective)\n",
        "problem.optimise()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, let's check it's found the correct result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    f\"x1: value: {parameterisation_2.variables['x1'].value}, upper_bound: {parameterisation_2.variables['x1'].upper_bound}\"\n",
        ")\n",
        "print(\n",
        "    f\"x2: value: {parameterisation_2.variables['x2'].value}, lower_bound: {parameterisation_2.variables['x2'].lower_bound}\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Now let's include a relatively arbitrary constraint:\n",
        "We're going to minimise length again, but with a constraint that says that we don't\n",
        "want the length to be below some arbitrary value of 50.\n",
        "There are much better ways of doing this, but this is to demonstrate the use of an\n",
        "inequality constraint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def calculate_constraint(vector, parameterisation, c_value):\n",
        "    \"\"\"\n",
        "    Some arbitrary constraint function\n",
        "    \"\"\"\n",
        "    parameterisation.variables.set_values_from_norm(vector)\n",
        "    length = parameterisation.create_shape().length\n",
        "    return np.array([c_value - length])\n",
        "\n",
        "\n",
        "def my_constraint(constraint, vector, grad, parameterisation, c_value, ad_args=None):\n",
        "    \"\"\"\n",
        "    Constraint function with numerical gradient calculation\n",
        "    \"\"\"\n",
        "    value = calculate_constraint(vector, parameterisation, c_value)\n",
        "    constraint[:] = value\n",
        "\n",
        "    if grad.size > 0:\n",
        "        grad[:] = approx_derivative(\n",
        "            calculate_constraint,\n",
        "            vector,\n",
        "            f0=value,\n",
        "            args=(parameterisation, c_value),\n",
        "            **ad_args,\n",
        "        )\n",
        "\n",
        "    return constraint\n",
        "\n",
        "\n",
        "c_value = 50\n",
        "c_tolerance = 1e-6\n",
        "constraint_function = OptimisationConstraint(\n",
        "    my_constraint,\n",
        "    f_constraint_args={\"parameterisation\": parameterisation_2, \"c_value\": c_value},\n",
        "    tolerance=np.array([c_tolerance]),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Setting up the problem with a constraint is the same as before, but with an additional\n",
        "argument"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parameterisation_3 = PrincetonD()\n",
        "slsqp_optimiser2 = Optimiser(\n",
        "    \"SLSQP\",\n",
        "    opt_conditions={\n",
        "        \"ftol_rel\": 1e-3,\n",
        "        \"xtol_rel\": 1e-12,\n",
        "        \"xtol_abs\": 1e-12,\n",
        "        \"max_eval\": 1000,\n",
        "    },\n",
        ")\n",
        "constraint_function = OptimisationConstraint(\n",
        "    my_constraint,\n",
        "    f_constraint_args={\"parameterisation\": parameterisation_3, \"c_value\": c_value},\n",
        "    tolerance=np.array([c_tolerance]),\n",
        ")\n",
        "objective = OptimisationObjective(\n",
        "    minimise_length,\n",
        "    f_objective_args={\"parameterisation\": parameterisation_3},\n",
        ")\n",
        "problem = GeometryOptimisationProblem(\n",
        "    parameterisation_3, slsqp_optimiser2, objective, constraints=[constraint_function]\n",
        ")\n",
        "problem.optimise()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Both x1 and x2 are free variables and between them they should be create a PrincetonD\n",
        "shape of length exactly 50 (as the bounds on these variables surely allow it).\n",
        "As we are minimising length, we'd expect to see a function value of 50 here (+/- the\n",
        "tolerances)... but we don't!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Theoretical optimum: {c_value-c_tolerance}\")\n",
        "print(f\"Length with SLSQP: {parameterisation_3.create_shape().length}\")\n",
        "print(f\"n_evals: {problem.opt.n_evals}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is because we're using numerical gradients and jacobians for our objective and\n",
        "inequality constraint functions. This can be faster than other approaches, but is less\n",
        "robust and also less likely to find the best solution.\n",
        "\n",
        "Let's try a few different optimisers, noting the different termination conditions we\n",
        "can play with and their effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parameterisation_4 = PrincetonD()\n",
        "cobyla_optimiser2 = Optimiser(\n",
        "    \"COBYLA\",\n",
        "    opt_conditions={\n",
        "        \"ftol_rel\": 1e-7,\n",
        "        \"xtol_rel\": 1e-12,\n",
        "        \"xtol_abs\": 1e-12,\n",
        "        \"max_eval\": 1000,\n",
        "    },\n",
        ")\n",
        "constraint_function = OptimisationConstraint(\n",
        "    my_constraint,\n",
        "    f_constraint_args={\"parameterisation\": parameterisation_4, \"c_value\": c_value},\n",
        "    tolerance=np.array([c_tolerance]),\n",
        ")\n",
        "objective = OptimisationObjective(\n",
        "    minimise_length,\n",
        "    f_objective_args={\"parameterisation\": parameterisation_4},\n",
        ")\n",
        "problem = GeometryOptimisationProblem(\n",
        "    parameterisation_4, cobyla_optimiser2, objective, constraints=[constraint_function]\n",
        ")\n",
        "problem.optimise()\n",
        "\n",
        "print(f\"Theoretical optimum: {c_value - c_tolerance}\")\n",
        "print(f\"Length with COBYLA: {parameterisation_4.create_shape().length}\")\n",
        "print(f\"n_evals: {problem.opt.n_evals}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "ISRES is a stochastic optimisation algorithm; if we want to see the same results every\n",
        "time, it's advisable to set the random seed to a known value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "set_random_seed(134365475)\n",
        "\n",
        "parameterisation_5 = PrincetonD()\n",
        "irses_optimiser = Optimiser(\n",
        "    \"ISRES\",\n",
        "    opt_conditions={\n",
        "        \"ftol_rel\": 1e-12,\n",
        "        \"xtol_rel\": 1e-12,\n",
        "        \"xtol_abs\": 1e-12,\n",
        "        \"max_eval\": 1000,\n",
        "    },\n",
        ")\n",
        "\n",
        "objective = OptimisationObjective(\n",
        "    minimise_length,\n",
        "    f_objective_args={\"parameterisation\": parameterisation_5},\n",
        ")\n",
        "constraint_function = OptimisationConstraint(\n",
        "    my_constraint,\n",
        "    f_constraint_args={\"parameterisation\": parameterisation_5, \"c_value\": c_value},\n",
        "    tolerance=np.array([c_tolerance]),\n",
        ")\n",
        "problem = GeometryOptimisationProblem(\n",
        "    parameterisation_5, irses_optimiser, objective, constraints=[constraint_function]\n",
        ")\n",
        "problem.optimise()\n",
        "\n",
        "print(f\"Theoretical optimum: {c_value - c_tolerance}\")\n",
        "print(f\"Length with ISRES: {parameterisation_5.create_shape().length}\")\n",
        "print(f\"n_evals: {problem.opt.n_evals}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Horses for courses folks... YMMV. Best thing you can do is specify your optimisation\n",
        "problem intelligently, using well-behaved objective and constraint functions, and smart\n",
        "bounds. Trying out different optimisers doesn't hurt. There's a trade-off between speed\n",
        "and accuracy. If you can't work out the analytical gradients, numerical gradients are a\n",
        "questionable approach, but can work well (fast) on some problems."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
